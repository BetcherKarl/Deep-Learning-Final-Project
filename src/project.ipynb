{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T21:46:21.799062Z",
     "start_time": "2024-04-15T21:46:21.787857Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow.keras as k\n",
    "\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# helper functions\n",
    "output_format = ['ace', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'jack', 'queen', 'king', 'clubs', 'spades', 'diamonds', 'hearts', 'joker']\n",
    "def parse_output(output: np.array) -> str:\n",
    "    \"\"\"\n",
    "    Parse the output from a list of numbers into a string like 'joker' or 'ace of hearts' or 'eight of spades'\n",
    "    :param output: the output from the DL model as a list of 18 numbers between 0 and 1\n",
    "    :return: string name of the card\n",
    "    \"\"\"\n",
    "    if output.shape != np.array(output_format).shape:\n",
    "        raise ValueError('output is not of the correct shape')\n",
    "    if max(output) == output[-1]:\n",
    "        return 'joker'\n",
    "    outputs = list(output)\n",
    "    rank_index = outputs[:13].index(max(output[:13]))\n",
    "    suit_index = outputs[13:17].index(max(output[13:17])) + 13\n",
    "    return f\"{output_format[rank_index]} of {output_format[suit_index]}\"    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T21:46:21.821784Z",
     "start_time": "2024-04-15T21:46:21.804085Z"
    }
   },
   "id": "fc7e7468651fa420",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 8.55 GiB for an array with shape (7624, 224, 224, 3) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[35], line 50\u001B[0m\n\u001B[0;32m     47\u001B[0m                 \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSomething went wrong with \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata/\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m+\u001B[39mrow[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and category \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrow[\u001B[38;5;241m4\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     48\u001B[0m                 exit(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 50\u001B[0m train_inputs \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_inputs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m255\u001B[39;49m\n\u001B[0;32m     51\u001B[0m train_outputs \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(train_outputs)\n\u001B[0;32m     52\u001B[0m validation_inputs \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(validation_inputs) \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m255\u001B[39m\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 8.55 GiB for an array with shape (7624, 224, 224, 3) and data type float64"
     ]
    }
   ],
   "source": [
    "# import and assemble data\n",
    "train_inputs = []\n",
    "train_outputs = []\n",
    "validation_inputs = []\n",
    "validation_outputs = []\n",
    "test_inputs = []\n",
    "test_outputs = []\n",
    "\n",
    "\n",
    "\n",
    "with open('data/cards.csv', 'r') as file:\n",
    "    csv_reader = csv.reader(file, delimiter=',')\n",
    "\n",
    "    flag = True\n",
    "    for row in csv_reader:\n",
    "        # print(row)\n",
    "        if flag:\n",
    "            headers = row\n",
    "            flag = False\n",
    "        else:\n",
    "            # determine input\n",
    "            img = np.asarray(Image.open('data/'+row[1]))\n",
    "            \n",
    "            # determine output\n",
    "            name = row[2]\n",
    "            output = [1 if title in name else 0 for title in output_format]\n",
    "                \n",
    "            if sum(output) == 2 and row[2] != 'joker': # suit and number activated\n",
    "                output = np.array(output)\n",
    "            elif sum(output) == 1 and row[2] == 'joker': # just joker activated\n",
    "                output = np.array(output)\n",
    "            else:\n",
    "                print(f\"Something went wrong with {'data/'+row[1]}.\")\n",
    "                exit(1)\n",
    "                \n",
    "            # assembling data\n",
    "            if 'train' == row[4]:\n",
    "                train_inputs.append(img)\n",
    "                train_outputs.append(output)\n",
    "            elif 'test' == row[4]:\n",
    "                test_inputs.append(img)\n",
    "                test_outputs.append(output)\n",
    "            elif 'valid' == row[4]:\n",
    "                validation_inputs.append(img)\n",
    "                validation_outputs.append(output)\n",
    "            else:\n",
    "                print(f\"Something went wrong with {'data/'+row[1]} and category {row[4]}\")\n",
    "                exit(1)\n",
    "                \n",
    "train_inputs = np.array(train_inputs) / 255\n",
    "train_outputs = np.array(train_outputs)\n",
    "validation_inputs = np.array(validation_inputs) / 255\n",
    "validation_outputs = np.array(validation_outputs)\n",
    "test_inputs = np.array(test_inputs) / 255\n",
    "test_outputs = np.array(test_outputs)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Data importing complete\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T21:46:38.743732Z",
     "start_time": "2024-04-15T21:46:21.822790Z"
    }
   },
   "id": "530abc8cb35c5502",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# check random samples\n",
    "import random\n",
    "\n",
    "samples = []\n",
    "index = random.randint(0, len(train_inputs)-1)\n",
    "while len(samples) < 25:\n",
    "    if index not in samples:\n",
    "        samples.append(index)\n",
    "    index = random.randint(0, len(train_inputs)-1)\n",
    "\n",
    "plt.figure(figsize=[10,10])\n",
    "counter = 1\n",
    "\n",
    "for index in samples:\n",
    "    print(train_inputs[index])\n",
    "    plt.subplot(5, 5, counter)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_inputs[index])\n",
    "    plt.xlabel(parse_output(train_outputs[index]))\n",
    "    counter += 1\n",
    "    \n",
    "plt.show()\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T21:46:38.747250Z",
     "start_time": "2024-04-15T21:46:38.745741Z"
    }
   },
   "id": "31867f2b5195ef3b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define regularization strength\n",
    "l1_strength = 0.001\n",
    "\n",
    "# Define model\n",
    "model = k.models.Sequential()\n",
    "\n",
    "model.add(k.Input(shape=(224, 224, 3)))\n",
    "model.add(k.layers.Conv2D(16, (15, 15), padding='same', strides=(3, 3), activation=k.layers.LeakyReLU(negative_slope=0.01), kernel_regularizer=k.regularizers.l1(l1_strength)))\n",
    "model.add(k.layers.BatchNormalization())\n",
    "model.add(k.layers.MaxPooling2D(pool_size=(7, 7)))\n",
    "model.add(k.layers.Dropout(0.3))\n",
    "\n",
    "model.add(k.layers.Flatten())\n",
    "model.add(k.layers.Dense(128, activation=k.layers.LeakyReLU(negative_slope=0.01), kernel_regularizer=k.regularizers.l1(l1_strength)))\n",
    "model.add(k.layers.Dense(128, activation=k.layers.LeakyReLU(negative_slope=0.01), kernel_regularizer=k.regularizers.l1(l1_strength)))\n",
    "\n",
    "model.add(k.layers.Dense(len(output_format), activation='softmax'))\n",
    "\n",
    "model.summary()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-15T21:46:38.748771Z"
    }
   },
   "id": "c42a169d5cbb457f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Learning rate scheduler\n",
    "learning_rate_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.9\n",
    ")\n",
    "\n",
    "# Create an optimizer with the learning rate schedule\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate_schedule)\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=opt, loss=k.losses.categorical_crossentropy, metrics=['accuracy'])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "299842584e516ac1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Train the model\n",
    "num_epochs = 25\n",
    "history = model.fit(\n",
    "    train_inputs, train_outputs,\n",
    "    epochs=num_epochs,\n",
    "    batch_size=32,\n",
    "    validation_data=(validation_inputs, validation_outputs),\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-15T21:46:38.751771Z"
    }
   },
   "id": "1ef360e330f549b9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d88534e8e8de504",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e499fade656784c7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c974ca840bd1a4e7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
